{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2.  Redes recurrentes sobre texto\n",
    "Hoy en dı́a, una aplicación relevante de las redes neuronales recurrentes es el modelamiento de texto y lenguaje natural. En esta sección abordaremos el problema de procesar sentencias de texto proporcionadas por GMB (*Groningen Meaning Bank*) para reconocimiento de entidades y tagger. Trabajaremos con el dataset proprocionado a través de la interfaz de Kaggle en el siguiente __[link](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)__, con mas de un millón de palabras trabajaremos este dataset para realizar predicciones sobre distintas tareas, del tipo *many to many* y *many to one*.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/b4sus.jpg\" width=\"70%\" />\n",
    "\n",
    "\n",
    "Descargue los datos de la página de Kaggle y cárgelos a través de *pandas*.\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_ner = pd.read_csv(\"./entity-annotated-corpus/ner.csv\", error_bad_lines=False)\n",
    "df_ner.dropna(inplace=True)\n",
    "```\n",
    "\n",
    "> a) En esta primera instancia trabajaremos con la tarea de realizar un POS *tag* (*Part of Speech*) sobre cada una de las palabras en las sentencias que se nos presenta en los datos, también puede intentar el NER (*Named Entity Recogntion*) sobre la columna *tag*, esta tarea es del tipo *many to many*, es decir, la entrada es una secuencia y la salida es una secuencia sin *shift*, por lo que necesitaremos una estructura de red adecuada a esto. En primer lugar extraiga las columnas que utilizaremos del dataset ¿Por qué es conveniente utilizar *lemma* en vez de la palabra misma *word*?\n",
    "```python\n",
    "dataset = df_ner.loc[:,[\"lemma\",\"pos\",\"tag\",\"prev-iob\"]]\n",
    "```\n",
    "Luego de esto cree una estructura que contendrá todas las sentencias u oraciones y otra estructura que contendrá los *pos tagger*, esto es un arreglo de arreglos de *lemmas* y un arreglo de arreglos de *tags* respectivamente. ¿Cuales son las dimensiones de ambas estructuras? ¿Cada dato de ejemplo tiene las mismas dimensiones que el resto?\n",
    "```python\n",
    "dataX,dataY = [],[]\n",
    "#uniques\n",
    "lemmas,labels = set(), set()\n",
    "for fila in dataset.values:\n",
    "    if fila[-1]==\"__START1__\": \n",
    "        dataX.append(np.asarray(sentence))\n",
    "        dataY.append(np.asarray(labels_sentence))\n",
    "        sentence= []\n",
    "        label_sentence = []\n",
    "    lemmas.add(fila[0])\n",
    "    labels.add(fila[1])\n",
    "    sentence.append(fila[0])#add lemma\n",
    "    labels_sentence.append(fila[1]) #POS o TAG\n",
    "#data to  array\n",
    "dataX = np.asarray(dataX[1:])\n",
    "dataY = np.asarray(dataY[1:])\n",
    "```    \n",
    "\n",
    "> b) Estudie la distribución del largo de los textos a procesar. Estudie también la frecuencia con la que aparecen\n",
    "las palabras en todo el dataset. ¿Se observa una ley Zipf? ¿Cambia el resultado cuando se separan los textos de acuerdo a su clase/categorı́a? Comente.\n",
    "\n",
    "> c) Es necesario transformar los textos para que puedan ser entregados apropiadamente a la red, por lo será necesario crear una función que codifique cada posible *lemma* a un número y cada posible *tag* a otro número, utilice esta función sobre las sentencias y *tags* ya generados. Mida cual es el largo máximo de entre todas las sentencias, la cantidad de *lemmas* y etiquetas. Además de esto, debido al largo distinto de las sentencias se deberá realizar *padding* para estandarizar el largo, considere algun carácter especial para codificar el espacio en blanco que luego se le deberá rellenar, por ejemplo si el largo máximo es de 4 y se tiene la sentencia \"the rocket\" codificada como [32,4] será necesario agregar un *lemma* que codificado significará el fin de la sentencia \"the rocket *ENDPAD ENDPAD*\" y codificado quedará como [32,4,*0, 0*].\n",
    "```python\n",
    "...#add fullfill lemma and tag to the dictionary\n",
    "lemma2idx = {w: i for i, w in enumerate(lemmas)} #Converting text to numbers\n",
    "lab2idx = {t: i for i, t in enumerate(labels)}\n",
    "dataX = [[lemma2idx[lemma] for lemma in sentence ] for sentence in dataX]\n",
    "dataY = [[lab2idx[pos] for pos in pos_tags ] for pos_tags in dataY]\n",
    "n_lemmas = len(lemmas)\n",
    "n_labels = len(labels)\n",
    "```\n",
    "\n",
    "> d) Realice el *padding* anteriormente mencionado, decida sobre qué le parece mas conveniente al rellenar con el valor especial ¿Al principio o al final de la sentencia? Comente\n",
    "```python\n",
    "from keras.preprocessing import sequence\n",
    "X = sequence.pad_sequences(dataX,maxlen=max_input_lenght,padding='post' or 'pre',value=lemma2idx[\"yourspecialcharacter\"]) \n",
    "y = sequence.pad_sequences(dataY,maxlen=max_input_lenght,padding='post' or 'pre',value=lab2idx[\"endtagger\"])\n",
    "```\n",
    "\n",
    "> e) Para el poder entregar una clasificación sobre los distintos *pos tagger* es necesario tranformarlas a *one hot vectors*, debido a que están codificadas en números enteros, con esto se quedará con un arreglo tridimensional con la cantidad de ejemplos, la cantidad máxima de palabras y la cantidad de posibles *pos tags*. Luego de esto cree los conjuntos de entrenamiento y de prueba con el código a continuación ¿Cuáles son las dimensiones de entrada y salida de cada conjunto? Comente\n",
    "```python\n",
    "from keras.utils import to_categorical\n",
    "y = np.asarray([to_categorical(i, num_classes=n_labels) for i in y])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=22)\n",
    "```\n",
    "\n",
    "> f) Defina una red neuronal recurrente *many to many* con compuertas LSTM para aprender a *tagear* el texto, entrenela y evalúe su desempeño sobre ambos conjuntos. Esta red debe procesar la secuencia de *lemmas* rellenados (o sin rellenar) y entregar el *pos tag* a cada uno de estos *lemmas*, por lo que la salida de la red no es un vector como anteriormente se ha trabajado, sino que tiene una dimensión extra la cual es debido a que en cada instante de tiempo se necesita entregar un *output*. Como los *lemmas* corresponden a datos esencialmente categóricos, o al menos discretos, es necesario generar una representación vectorial de ellas. La primera capa de la red a construir debe por lo tanto incluir una transformación entrenable desde el espacio de representación original (discreto) a ${\\rm I\\!R}^{d}$ , con $d$ la dimensionalidad del *embedding*. Comente sobre los cambios que sufre un dato al ingresar a la red y la cantidad de parámetros de la red.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "embedding_vector = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_vector, input_length=max_input_lenght))\n",
    "model.add(LSTM(units=100,return_sequences=True))\n",
    "model.add(Dense(n_labels, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=128)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "```\n",
    "\n",
    "> g) Varı́e la dimensionalidad del embedding inicial y determine si aumenta o disminuye el error de clasificación. Comente.\n",
    "\n",
    "> h) Use Dropout para entrenar la LSTM. ¿El Dropout mejora el desempeño de la red? Señale cuales podrı́an ser las causas del comportamiento observado.\n",
    "```python\n",
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_vector, input_length=max_input_lenght))\n",
    "model.add(LSTM(units=100,return_sequences=True)) #or recurrent_dropout=0.2\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_labels, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=128)\n",
    "```\n",
    "\n",
    "> i) Algunos autores señalan la importante dependencia que existe en texto, no solo con las palabras anteriores, sino que con las que siguen. Mejore la red definida en f) utilizando una red neuronal recurrente Bidireccional, es decir, con recurrencia en ambas direcciones sobre la secuencia de *lemmas* de entrada. Comente cuál debiera ser la forma correcta de usar el parámetro *merge_mode* (concatenar, multiplicar, sumar o promediar) para este caso. Además comente las transformaciones que sufre el patrón de entrada al pasar por las capas. ¿Mejora o empeora el desempeño? Analice.\n",
    "```python\n",
    "from keras.layers import Bidirectional\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_vector, input_length=max_input_lenght))\n",
    "layer_lstm = LSTM(units=100,return_sequences=True)\n",
    "model.add(Bidirectional(layer_lstm,merge_mode=choose))\n",
    "model.add(Dense(n_labels, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=128)\n",
    "```\n",
    "\n",
    "> j) Utilice alguna de las red entrenadas, ojalá una con buen desempeño y muestre las predicciones, el *pos tager*, sobre algún ejemplo de pruebas, comente. Para entender qué son los símbolos *Part of speech tags* visite el siguiente link: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html \n",
    "```python\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15}: {}\".format(\"Lemma\", \"Pred\"))\n",
    "for w,pred in zip(X_test[i],p[0]):\n",
    "    print(\"{:15}: {}\".format(lemmas[w],labels[pred]))\n",
    "```\n",
    "\n",
    "Ahora utilizaremos el mismo dataset para realizar una aplicación mas conocida hoy en día que es el autocompletar texto, esto es, predecir la siguiente palabra de una sentencia basada en las palabras anteriores de la misma, por lo que la red que utilizaremos es del tipo *many to one*.  \n",
    "Debido a lo extenso del vocabulario es bastante complejo hacer un modelo que prediga una palabra dentro de las millones que pueden haber, por lo que, trabajaremos a nivel de carácter, en donde las posibilidades (posibles clases) son mucho menores.\n",
    "\n",
    "> k) Carge las palabras del dataset ¿Por qué no los *lemmas*? y cree el corpus con el cual se trabajará, además de crear la codificación de caracteres a números. Esto se presenta en el código a continuación además de crear la estrucutura de los datos con los que se va a trabajar (sub sentencias del corpus original). Utilice el tamaño del *corpus* que le acomode a la memoria de su computador.\n",
    "```python\n",
    "dataset = df_ner.loc[:,[\"word\",\"lemma\"]]\n",
    "text = ' '.join(dataset[\"word\"]).lower() #corpus\n",
    "null_character = \"*\"\n",
    "chars = [null_character]+sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 5 \n",
    "sentences = []\n",
    "next_chars = []\n",
    "size = int(len(text)*0.2) #solo un 20% del corpus\n",
    "for i in range(0, size - maxlen, step):\n",
    "    sentences.append(null_character+text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "```\n",
    "\n",
    "> l) Procese las sentencias para así tenerlas codificadas en números que van a representar los carácteres, tal cual se realizó en c) con los *lemmas*, lo mismo para las etiquetas. Además de esto deberá realizar el *padding* correspondiente al comienzo de la sentencia, esto es para que la red aprenda cuando venga una frase mas corta de lo entrenado, este símbolo siignificará que no hay información. Transforme las etiquetas a *one hot vector* como se realizó en c) y defina la red similar a la presentada en f), con un *embedding* seguido de una capa recurrente GRU y la capa de clasificación. Aprovechese de la implementación más rápida de GRU respaldada por __[CuDNN](https://developer.nvidia.com/cudnn)__, una librería de CUDA (NVIDIA) para *Deep Neural Network*. \n",
    "```python\n",
    "dataX = [[char_indices[char] for char in sentence ] for sentence in sentences]\n",
    "dataY = [char_indices[char] for char in next_chars]\n",
    "...#dataX pad sequence padding='pre'\n",
    "...#dataY to categorical with num_classes=len(chars)\n",
    "from keras.layers import CuDNNGRU,GRU\n",
    "embedding_vector = 16\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(chars), output_dim=embedding_vector, input_length=maxlen+1))#\n",
    "model.add(CuDNNGRU(units=512,return_sequences=False)) #or GRU\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    ">  m) Entrene la red con las funciones que se presentan a continuación que mostrarán el cómo va la tarea de autocompletar texto en cada *epoch*, generando una sentencia completa de 400 carácteres *aleatoriamente* a partir de una semilla *random*. Entrene solo durante 25 *epochs*, a los 15 ya debería comenzar a generar palabras y sonar mas coherente.\n",
    "```python\n",
    "def predict_next_char(model, sentence, diversity=1.0):\n",
    "    \"\"\"Predict the next character from the current one\"\"\"    \n",
    "    x_pred = [char_indices[null_character]]+[char_indices[char] for char in sentence]\n",
    "    x_pred = sequence.pad_sequences([x_pred], maxlen=maxlen+1,padding='pre',value=char_indices[null_character])\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = np.random.choice(len(chars), p=preds)\n",
    "    return indices_char[next_index]\n",
    "import random,sys\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print('\\n----- Generating text after Epoch: %d' % epoch)\n",
    "    start_index = random.randint(0, size - maxlen - 1)\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(sentence)\n",
    "    for i in range(400):\n",
    "        next_char = predict_next_char(model, sentence0)\n",
    "        sentence = sentence[1:] + next_char #for next character\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return\n",
    "from keras.callbacks import LambdaCallback\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "model.fit(X, y,batch_size=256,epochs=25, callbacks=[print_callback])\n",
    "```\n",
    "\n",
    "> n) Verifique la calidad de la red entrenada, cargando el modelo si es que lo guardó o directamente, entregando una predicción sobre una semilla inicial que usted entregue. Observe y comente cualitativamente sobre qué pasa cuando la predicción del siguiente carácter fuese de manera determinista, tomando el máximo valor de entre las predicciones.\n",
    "```python\n",
    "sentence = \"it is \"\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(sentence)\n",
    "for i in range(400):\n",
    "    next_char = predict_next_char(model, sentence)\n",
    "    sentence = sentence[1:] + next_char \n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
